<!-- HTML header for doxygen 1.11.0-->
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US" class="dark-mode">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=11"/>
<meta name="generator" content="Doxygen 1.11.0"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>Photon: Feature Overview</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<script type="text/javascript" src="clipboard.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="cookie.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  extensions: ["tex2jax.js"],
  jax: ["input/TeX","output/HTML-CSS"],
});
</script>
<script type="text/javascript" async="async" src="https://cdn.jsdelivr.net/npm/mathjax@2/MathJax.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="doxygen-awesome.css" rel="stylesheet" type="text/css"/>
<link href="custom.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="doxygen-awesome-interactive-toc.js"></script>
<script type="text/javascript">
    DoxygenAwesomeInteractiveToc.init()
</script>
</head>
<body>
<!-- Add a link to project GitHub by https://tholman.com/github-corners/ -->
<a href="https://github.com/TzuChieh/Photon-v2" class="github-corner" aria-label="View source on GitHub"><svg width="80" height="80" viewBox="0 0 250 250" style="fill:#FD6C6C; color:#fff; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr id="projectrow">
  <td id="projectalign">
   <div id="projectname">Photon<span id="projectnumber">&#160;2.0.0-beta</span>
   </div>
   <div id="projectbrief">A physically based renderer.</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.11.0 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
var searchBox = new SearchBox("searchBox", "search/",'.html');
/* @license-end */
</script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function() { codefold.init(0); });
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function() {
  initMenu('',true,false,'search.php','Search',true);
  $(function() { init_search(); });
});
/* @license-end */
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function(){initNavTree('main_feature_overview.html',''); initResizable(true); });
/* @license-end */
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<div id="MSearchResults">
<div class="SRPage">
<div id="SRIndex">
<div id="SRResults"></div>
<div class="SRStatus" id="Loading">Loading...</div>
<div class="SRStatus" id="Searching">Searching...</div>
<div class="SRStatus" id="NoMatches">No Matches</div>
</div>
</div>
</div>
</div>

<div><div class="header">
  <div class="headertitle"><div class="title">Feature Overview</div></div>
</div><!--header-->
<div class="contents">
<div class="toc"><h3>Table of Contents</h3>
<ul><li class="level1"><a href="#autotoc_md18">Visualizer</a><ul><li class="level2"><a href="#bvpt">Unidirectional Path Tracing</a></li>
<li class="level2"><a href="#bneept">Unidirectional Path Tracing with NEE</a></li>
<li class="level2"><a href="#vpm">Vanilla Photon Mapping</a></li>
<li class="level2"><a href="#ppm">Progressive Photon Mapping</a></li>
<li class="level2"><a href="#sppm">Stochastic Progressive Photon Mapping</a></li>
<li class="level2"><a href="#pppm">Probabilistic Progressive Photon Mapping</a></li>
<li class="level2"><a href="#autotoc_md19">Scheduler</a></li>
</ul>
</li>
<li class="level1"><a href="#observer">Observer</a><ul><li class="level2"><a href="#pinhole">Pinhole Camera</a></li>
<li class="level2"><a href="#thinlens">Thin Lens Camera</a></li>
<li class="level2"><a href="#autotoc_md20">Environment Camera</a></li>
<li class="level2"><a href="#autotoc_md21">Energy Measurement</a></li>
</ul>
</li>
<li class="level1"><a href="#autotoc_md22">Image</a><ul><li class="level2"><a href="#autotoc_md23">Constant Image</a></li>
<li class="level2"><a href="#autotoc_md24">Raster Image</a></li>
<li class="level2"><a href="#autotoc_md25">Procedural Image</a></li>
<li class="level2"><a href="#autotoc_md26">Black-body Radiation</a></li>
</ul>
</li>
<li class="level1"><a href="#autotoc_md27">Film</a><ul><li class="level2"><a href="#autotoc_md28">Filtering</a></li>
<li class="level2"><a href="#autotoc_md29">Tone-mapping</a></li>
<li class="level2"><a href="#autotoc_md30">Reading and Writing</a></li>
</ul>
</li>
<li class="level1"><a href="#material">Material</a><ul><li class="level2"><a href="#autotoc_md31">Matte Opaque</a></li>
<li class="level2"><a href="#autotoc_md32">Ideal Substance</a></li>
<li class="level2"><a href="#abraded_opaque">Abraded Opaque</a></li>
<li class="level2"><a href="#abraded_translucent">Abraded Translucent</a></li>
<li class="level2"><a href="#autotoc_md33">Binary Mixed Surface</a></li>
<li class="level2"><a href="#autotoc_md34">Layered Material</a></li>
</ul>
</li>
<li class="level1"><a href="#geometry">Geometry</a><ul><li class="level2"><a href="#autotoc_md35">Intersectable and Primitive</a></li>
<li class="level2"><a href="#autotoc_md36">Basic Shapes</a></li>
<li class="level2"><a href="#autotoc_md37">Masking</a></li>
<li class="level2"><a href="#autotoc_md38">Advanced Shapes</a></li>
<li class="level2"><a href="#autotoc_md39">Acceleration Structure</a></li>
</ul>
</li>
<li class="level1"><a href="#autotoc_md40">Light</a><ul><li class="level2"><a href="#autotoc_md41">Area Light</a></li>
<li class="level2"><a href="#autotoc_md42">Point Light</a></li>
<li class="level2"><a href="#autotoc_md43">Model Light</a></li>
<li class="level2"><a href="#autotoc_md44">IES Light Profiles</a></li>
<li class="level2"><a href="#autotoc_md45">Sky Dome</a></li>
</ul>
</li>
<li class="level1"><a href="#autotoc_md46">Sample Source</a><ul><li class="level2"><a href="#autotoc_md47">Sample Generator</a></li>
</ul>
</li>
<li class="level1"><a href="#autotoc_md48">Toolset</a><ul><li class="level2"><a href="#autotoc_md49">Film Merger</a></li>
</ul>
</li>
</ul>
</div>
<div class="textblock"><p><a class="anchor" id="md__documentation_2feature__overview"></a></p>
<p>A quick overview of the features supported by Photon. Similar to <a class="el" href="main_project_structure.html#md__documentation_2project__structure">Project Structure</a>, this page can also serve as a handy reference for locating the necessary documentation or source code.</p>
<h1><a class="anchor" id="autotoc_md18"></a>
Visualizer</h1>
<p>Rendering is the process of visualizing a scene. In Photon, we utilize a <a class="elRef" href="../../Engine/html/classph_1_1_visualizer.html">Visualizer</a> to generate a <a class="elRef" href="../../Engine/html/classph_1_1_renderer.html">Renderer</a>, which is responsible for creating a rendered image. This is where most of the core algorithms meet and work together, making it arguably the most complex part of the render engine. Photon offers a variety of renderers, each tailored to handle specific scenarios. There is no one-size-fits-all renderer for any types of scenes, as the effectiveness of a rendering algorithm depends heavily on the scene's characteristics, such as geometry dimensions and material properties. At the extreme end, one can always construct a scene that will fail a certain rendering algorithm. Therefore, it is crucial to understand and experiment with different renderers when faced with rendering challenges.</p>
<h2><a class="anchor" id="bvpt"></a>
Unidirectional Path Tracing</h2>
<div class="image">
<img src="055_books_based_on_Libri_sulla_mensola_books_by_archemi.jpg" alt=""/>
<div class="caption">
An example image rendered using this renderer.</div></div>
    <p>This is a relatively straightforward approach to solving the <em>rendering equation</em> as proposed by Kajiya <a class="el" href="citelist.html#CITEREF_Kajiya:1986:Rendering">[11]</a>, and is often a good starting point before diving into more complex solvers. Backward unidirectional path tracing, however, has its limitations. It does not handle scenes with small light sources or scenes where light transport paths are difficult to trace. This renderer can be constructed by creating a <a class="elRef" href="../../Engine/html/classph_1_1_path_tracing_visualizer.html">PathTracingVisualizer</a> paired with <a class="elRef" href="../../Engine/html/namespaceph.html#ad2b2972641b7279cac3ed861bfd53962">BVPT</a> energy estimator. If the rendering focuses solely on direct lighting, you may opt for the <a class="elRef" href="../../Engine/html/namespaceph.html#ad2b2972641b7279cac3ed861bfd53962">BVPTDL</a> estimator instead.</p>
<h2><a class="anchor" id="bneept"></a>
Unidirectional Path Tracing with NEE</h2>
<div class="image">
<img src="bathroom_based_on_Salle_de_bain_by_nacimus.jpg" alt=""/>
<div class="caption">
NEE can handle more complex scenes.</div></div>
    <p>With the addition of Next Event Estimation (NEE), unidirectional path tracing can become more adept at handling complex scenes. While this method largely builds on the same principle as the non-NEE variant, this renderer incorporates additional sampling techniques, as described by Veach <a class="el" href="citelist.html#CITEREF_Veach:1995:Optimally">[19]</a>, to improve the rendering of scenes with smaller light sources (as these scenes are often challenging for unidirectional path tracing). In a similar way, create a <a class="elRef" href="../../Engine/html/classph_1_1_path_tracing_visualizer.html">PathTracingVisualizer</a> with <a class="elRef" href="../../Engine/html/namespaceph.html#ad2b2972641b7279cac3ed861bfd53962">BNEEPT</a> energy estimator to leverage the benefits of NEE.</p>
<h2><a class="anchor" id="vpm"></a>
Vanilla Photon Mapping</h2>
<div class="image">
<img src="vpm_perfumes_by_gp3991.jpg" alt=""/>
<div class="caption">
Perfume bottle and its cap rendered with 500M photons (radius: 0.1, spp: 4). The image has not rendered to convergence to better show the characteristics between different photon mapping techniques.</div></div>
    <p>It is worth to clarify that the project is named <em>Photon</em> not because the focus is on photon mapping techniques, but because photon is the elementary particle that transmit light, and the primary aim of this project is to solve light transport problems. That said, Photon does offer several photon mapping techniques, with the most fundamental one being vanilla photon mapping as introduced by Jensen <a class="el" href="citelist.html#CITEREF_Jensen:1996:Global">[10]</a>. Photon mapping techniques are distinguished by their ability to <em>merge</em> nearby light transport paths, and this form of path reuse allows them to excel on a wider range of lighting phenomena. You can get a vanilla photon mapping renderer by creating a <a class="elRef" href="../../Engine/html/classph_1_1_photon_mapping_visualizer.html">PhotonMappingVisualizer</a> and set it to <a class="elRef" href="../../Engine/html/namespaceph.html#acae1a4214e781895f3275c4923f42faa">Vanilla</a> mode.</p>
<h2><a class="anchor" id="ppm"></a>
Progressive Photon Mapping</h2>
<div class="image">
<img src="ppm_perfumes_by_gp3991.jpg" alt=""/>
<div class="caption">
Perfume bottle and its cap rendered with 0.5M photons for 1000 passes (radius: 0.1, spp: 4). The image has not rendered to convergence to better show the characteristics between different photon mapping techniques.</div></div>
    <p>This renderer implements the PPM algorithm by Hachisuka et al. <a class="el" href="citelist.html#CITEREF_Hachisuka:2008:Progressive">[7]</a> This method improves upon vanilla photon mapping by tracing a fixed set of view vertices as the first pass, then progressively refine the estimation of the incoming energy on those vertices. The rendered result will be closer to ground truth with more view vertices (and the above image is rendered with maximum view path length = 10).</p>
<p>One of the key advantages of progressive photon mapping methods is their ability to utilize an effectively infinite number of photons, which allows it to handle complex lighting scenarios more effectively than vanilla photon mapping. This capability leads to superior rendering results, especially for scenes dominated by specular-diffuse-specular paths. Create a <a class="elRef" href="../../Engine/html/classph_1_1_photon_mapping_visualizer.html">PhotonMappingVisualizer</a> in <a class="elRef" href="../../Engine/html/namespaceph.html#acae1a4214e781895f3275c4923f42faa">Progressive</a> mode to use this renderer.</p>
<h2><a class="anchor" id="sppm"></a>
Stochastic Progressive Photon Mapping</h2>
<div class="image">
<img src="sppm_perfumes_by_gp3991.jpg" alt=""/>
<div class="caption">
Perfume bottle and its cap rendered with 0.5M photons for 1000 passes (radius: 0.1, spp: 4). The image has not rendered to convergence to better show the characteristics between different photon mapping techniques.</div></div>
    <p>This renderer implements the SPPM algorithm by Hachisuka et al. <a class="el" href="citelist.html#CITEREF_Hachisuka:2009:Stochastic">[6]</a>, and is intended to serve as a reference implementation for more sophisticated techniques. The SPPM (and PPM) implementation in Photon tries to avoid tricks and shortcuts where possible, so the evaluated radiance is more likely to remain correct as the project evolves. As a result, this renderer is not optimized for speed and may not be suitable for scenarios where rendering time is a critical factor. Create a <a class="elRef" href="../../Engine/html/classph_1_1_photon_mapping_visualizer.html">PhotonMappingVisualizer</a> in <a class="elRef" href="../../Engine/html/namespaceph.html#acae1a4214e781895f3275c4923f42faa">Progressive</a> mode to use this renderer.</p>
<h2><a class="anchor" id="pppm"></a>
Probabilistic Progressive Photon Mapping</h2>
<div class="image">
<img src="pppm_perfumes_by_gp3991.jpg" alt=""/>
<div class="caption">
Perfume bottle and its cap rendered with 0.5M photons for 1000 passes (radius: 0.1, spp: 4). The image has not rendered to convergence to better show the characteristics between different photon mapping techniques.</div></div>
    <p>This renderer is based on Knaus and Zwicker's paper <a class="el" href="citelist.html#CITEREF_Knaus:2011:Progressive">[12]</a>, which innovatively decouples any intermediate radiance estimate pass from its previous iterations. By isolating each pass's calculations, we can treat any photon mapping technique as a black box and parallelizes the calculation of each pass trivially. However, this approach comes with a trade-off: increased memory consumption. Because each iteration is treated independently, more memory is required to store the intermediate results. This makes our implementation of PPPM more memory-intensive compared to other photon mapping techniques. Create a <a class="elRef" href="../../Engine/html/classph_1_1_photon_mapping_visualizer.html">PhotonMappingVisualizer</a> in <a class="elRef" href="../../Engine/html/namespaceph.html#acae1a4214e781895f3275c4923f42faa">Progressive</a> mode to use this renderer.</p>
<h2><a class="anchor" id="autotoc_md19"></a>
Scheduler</h2>
<p>Some <a class="elRef" href="../../Engine/html/classph_1_1_visualizer.html">Visualizer</a>s can make use of a <a class="elRef" href="../../Engine/html/classph_1_1_work_scheduler.html">WorkScheduler</a> to distribute rendering work across multiple processor cores. Different types of schedulers dispatch their work in different ways. When rendering an image, a scheduler will typically use the dimensions of the image and number of iterations to define the total volume of work.</p>
<table class="markdownTable">
<tr class="markdownTableHead">
<th class="markdownTableHeadCenter">Scheduler Type   </th><th class="markdownTableHeadCenter">Dispatch Pattern    </th></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyCenter"><a class="elRef" href="../../Engine/html/classph_1_1_plate_scheduler.html">Bulk</a>: Layer by layer, each layer convers the whole image.   </td><td class="markdownTableBodyCenter"><img src="bulk_concept.svg" alt="" width="50%" style="pointer-events: none;" class="inline" title="Bulk Scheduler"/>        </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyCenter"><a class="elRef" href="../../Engine/html/classph_1_1_stripe_scheduler.html">Stripe</a>: Divide image into stripes.   </td><td class="markdownTableBodyCenter"><img src="stripe_concept.svg" alt="" width="50%" style="pointer-events: none;" class="inline" title="Stripe Scheduler"/>        </td></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyCenter"><a class="elRef" href="../../Engine/html/classph_1_1_grid_scheduler.html">Grid</a>: Step through evenly divided image row by row.   </td><td class="markdownTableBodyCenter"><img src="grid_concept.svg" alt="" width="50%" style="pointer-events: none;" class="inline" title="Grid Scheduler"/>        </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyCenter"><a class="elRef" href="../../Engine/html/classph_1_1_tile_scheduler.html">Tile</a>: Step through divided image row by row.   </td><td class="markdownTableBodyCenter"><img src="tile_concept.svg" alt="" width="50%" style="pointer-events: none;" class="inline" title="Tile Scheduler"/>        </td></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyCenter"><a class="elRef" href="../../Engine/html/classph_1_1_spiral_scheduler.html">Spiral</a>: Step through the image cell by cell in a spiral shape.   </td><td class="markdownTableBodyCenter"><img src="spiral_concept.svg" alt="" width="50%" style="pointer-events: none;" class="inline" title="Spiral Scheduler"/>        </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyCenter"><a class="elRef" href="../../Engine/html/classph_1_1_spiral_grid_scheduler.html">SpiralGrid</a>: Similar to spiral scheduler, except that each cell is subdivided into smaller cells.   </td><td class="markdownTableBodyCenter"><img src="spiral_grid_concept.svg" alt="" width="50%" style="pointer-events: none;" class="inline" title="Spiral Grid Scheduler"/>       </td></tr>
</table>
<h1><a class="anchor" id="observer"></a>
Observer</h1>
<p><a class="elRef" href="../../Engine/html/classph_1_1_observer.html">Observer</a> plays an important role in capturing and recording a scene: it intercepts incoming energy and record it on its sensing device called <a class="elRef" href="../../Engine/html/classph_1_1_film.html">Film</a>&ndash;so you can observe the virtual world that has been rendered. Observers can generate energy <a class="elRef" href="../../Engine/html/classph_1_1_receiver.html">Receiver</a>s. A common example of an energy receiver is camera, which senses and records lighting as a photograph.</p>
<h2><a class="anchor" id="pinhole"></a>
Pinhole Camera</h2>
<div class="image">
<img src="pinhole_camera_teaser.jpg" alt=""/>
<div class="caption">
Image rendered by a pinhole camera. Notice the sharpness in foreground and background; there is no difference and it appears to be in focus everywhere.</div></div>
    <p>Photon currently supports two types of perspective cameras: <a class="elRef" href="../../Engine/html/classph_1_1_pinhole_camera.html">PinholeCamera</a> and <a class="elRef" href="../../Engine/html/classph_1_1_thin_lens_camera.html">ThinLensCamera</a>. A pinhole camera is simply composed of a small aperture (the "pinhole", which serves as its lens system) and a film. Images captured by this camera is similar to how a normal human perceives the world but with several simplifications. Due to its simplicity, it is widely adopted in computer graphics industry. This kind of camera can be generated by creating a <a class="elRef" href="../../Engine/html/classph_1_1_single_lens_observer.html">SingleLensObserver</a> with a zero lens radius.</p>
<h2><a class="anchor" id="thinlens"></a>
Thin Lens Camera</h2>
<div class="image">
<img src="thinlens_camera_teaser.jpg" alt=""/>
<div class="caption">
Image rendered by a thin-lens camera. Notice the depth of field effect.</div></div>
    <p>For thin lens camera, as its name suggests, the lens system in this camera is assumed to be a single lens with negligible thickness. The biggest advantage of it is that depth of field effects are possible under this model (without introducing too many complexities). In the above render, depth of field is achieved by a 72 mm lens focusing on the first metallic monkey. This camera can be created similarly as <a class="el" href="#pinhole">pinhole camera</a> by using a non-zero lens radius.</p>
<h2><a class="anchor" id="autotoc_md20"></a>
Environment Camera</h2>
<h2><a class="anchor" id="autotoc_md21"></a>
Energy Measurement</h2>
<h1><a class="anchor" id="autotoc_md22"></a>
Image</h1>
<p><a class="elRef" href="../../Engine/html/classph_1_1_image.html">Image</a> is a data storing medium that can bring finer details onto almost every aspects of the scene. <a class="elRef" href="../../Engine/html/classph_1_1_t_texture.html">Textures</a> generated from an image can be categorized into <em>numeric</em> and <em>color</em>. Data sampled from a color texture will automatically adapt to the current spectral representation used by the renderer, while numeric textures will generally pass the data through without any changes.</p>
<h2><a class="anchor" id="autotoc_md23"></a>
Constant Image</h2>
<p>One of the most frequently used image is <a class="elRef" href="../../Engine/html/classph_1_1_constant_image.html">constant</a>. As suggested by its name, its value does not vary across the full image domain. A constant image takes an array of values as input and will not perform any transformation if color space information is not specified. The number of values in the input array also matters. For example, a single element will be interpreted as a constant spectrum when generating color textures, while three elements will be treated as a tristimulus color when spectral upsampling is performed. When conditions are met, the input array can also represent wavelength-value data points. Due to its versatility, its behavior is slightly complex, and the image implementation will try to detect and report potential issues to logs.</p>
<h2><a class="anchor" id="autotoc_md24"></a>
Raster Image</h2>
<p>Photographs are probably the most common images that we encounter in real life. Most photographs nowadays are converted to a grid of pixel values before being displayed on a monitor. The <a class="elRef" href="../../Engine/html/classph_1_1_raster_file_image.html">raster image</a> works similarly by reading pictures stored on disk into memory, converting them to pixel-based textures, and then mapping them to the scene to add details. We support many <a class="elRef" href="../../Engine/html/namespaceph.html#ad8edde251ef79a653acc8df14db78b9e">LDR and HDR formats</a> through this image interface.</p>
<div class="image">
<img src="raster_mountains.jpg" alt="" width="80%"/>
<div class="caption">
Using raster image as a map for radiance values on a light source.</div></div>
    <h2><a class="anchor" id="autotoc_md25"></a>
Procedural Image</h2>
<h2><a class="anchor" id="autotoc_md26"></a>
Black-body Radiation</h2>
<h1><a class="anchor" id="autotoc_md27"></a>
Film</h1>
<h2><a class="anchor" id="autotoc_md28"></a>
Filtering</h2>
<h2><a class="anchor" id="autotoc_md29"></a>
Tone-mapping</h2>
<h2><a class="anchor" id="autotoc_md30"></a>
Reading and Writing</h2>
<h1><a class="anchor" id="material"></a>
Material</h1>
<p>We present material implementations with a professional test scene standardized by André Mazzone and Chris Rydalch <a class="el" href="citelist.html#CITEREF_Mazzone:2023:Standard">[14]</a>. This scene is specifically designed to facilitate the observation and comparison of material appearances. There is an inset object within the main model that has a 18% albedo (this value is the current VFX industry standard), which serves as a neutral color comparator. For scene dimensions and other parameters, the <a href="https://github.com/usd-wg">Universal Scene Description Working Group</a> has a <a href="https://github.com/usd-wg/assets/tree/main/full_assets/StandardShaderBall">nice entry</a> for it, which is more up-to-date than the <a href="https://dl.acm.org/doi/10.1145/3610543.3626181">original paper</a>.</p>
<p>Materials are represented by <a class="elRef" href="../../Engine/html/classph_1_1_material.html">material data containers</a>. These containers generate <a class="elRef" href="../../Engine/html/classph_1_1_surface_optics.html">SurfaceOptics</a> to describe <a class="elRef" href="../../Engine/html/classph_1_1_surface_behavior.html">SurfaceBehavior</a> and <a class="elRef" href="../../Engine/html/classph_1_1_volume_optics.html">VolumeOptics</a> to describe <a class="elRef" href="../../Engine/html/classph_1_1_volume_behavior.html">VolumeBehavior</a>. Surface and volume behavior objects are typically defined with respect to a <a class="elRef" href="../../Engine/html/classph_1_1_primitive.html">Primitive</a>. See the <a class="el" href="#geometry">Geometry</a> section for more details.</p>
<h2><a class="anchor" id="autotoc_md31"></a>
Matte Opaque</h2>
<p>A classic material that is present in almost every renderer, whether offline or real-time, is the Lambertian-based diffuse BRDF, first described over 200 years ago by Lambert <a class="el" href="citelist.html#CITEREF_Lambert:1760:Photometria">[13]</a>. A generalization of Lambert's model is also implemented, which is often referred to as the Oren-Nayar reflectance model <a class="el" href="citelist.html#CITEREF_Oren:1994:Generalization">[15]</a>. This model describes the surface as a collection of small Lambertian facets, each having a different orientation. We implement these material models as <a class="elRef" href="../../Engine/html/classph_1_1_matte_opaque.html">MatteOpqaue</a>. Some examples are shown below:</p>
<table class="markdownTable">
<tr class="markdownTableHead">
<th class="markdownTableHeadCenter"><img src="lambertian_diffuse_0p5.jpg" alt="" class="inline" title="Lambertian Material"/>       </th><th class="markdownTableHeadCenter"><img src="oren_nayar_0p5_120deg.jpg" alt="" class="inline" title="Oren Nayar Material"/>        </th></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyCenter" colspan="2"><b>Left: Lambertian diffuse BRDF with 50% albedo. Right: Oren-Nayar diffuse BRDF with 50% albedo and 120° facet standard deviation.</b>   </td></tr>
</table>
<h2><a class="anchor" id="autotoc_md32"></a>
Ideal Substance</h2>
<p>We can simulate some materials that do not exist in the real world. One common example is the idealization of surface roughness, where a perfectly smooth interface is modeled. <a class="elRef" href="../../Engine/html/classph_1_1_ideal_substance.html">IdealSubstance</a> makes the visualization of these kinds of materials possible, which is of great interest for both theoretical and practical applications.</p>
<table class="markdownTable">
<tr class="markdownTableHead">
<th class="markdownTableHeadCenter"><img src="ideal_absorber.jpg" alt="" class="inline" title="Ideal Absorber"/>       </th><th class="markdownTableHeadCenter"><img src="ideal_reflector.jpg" alt="" class="inline" title="Ideal Reflector"/>        </th></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyCenter" colspan="2"><b>Left: Ideal absorber (absorbing 100% energy). Right: Ideal reflector (reflecting 100% energy).</b>   </td></tr>
</table>
<dl class="section note"><dt>Note</dt><dd>A material that reflects all energy would require an index of refraction \( \eta = \infty \), which may cause some numerical issues. A nice workaround is to use <a class="elRef" href="../../Engine/html/classph_1_1_schlick_approx_conductor_fresnel.html">Schlick's approximation</a> <a class="el" href="citelist.html#CITEREF_Schlick:1994:BRDF">[17]</a> with \( f_0 = 1 \).</dd></dl>
<p>Photon also supports tinting reflectance and transmittance with user-specified values. Note that this is not physically correct, and most of the color of dielectrics comes from internal volume absorption, not from interfaces. This feature is implemented for performance and artistic reasons only.</p>
<table class="markdownTable">
<tr class="markdownTableHead">
<th class="markdownTableHeadCenter"><img src="ideal_dielectric.jpg" alt="" class="inline" title="Ideal Dielectric"/>       </th><th class="markdownTableHeadCenter"><img src="ideal_dielectric_blue_refl_tint.jpg" alt="" class="inline" title="Ideal Dielectric Blue Reflection Tint"/>        </th></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyCenter" colspan="2"><b>Left: Ideal glass without any tint (index of refraction = 1.51714). Right: Ideal glass with blue reflection tint.</b>   </td></tr>
</table>
<table class="markdownTable">
<tr class="markdownTableHead">
<th class="markdownTableHeadCenter"><img src="ideal_dielectric_green_refr_tint.jpg" alt="" class="inline" title="Ideal Substance Green Refraction Tint"/>       </th><th class="markdownTableHeadCenter"><img src="ideal_dielectric_blue_refl_green_refr_tint.jpg" alt="" class="inline" title="Ideal Dielectric Blue Reflection Green Transmission Tint"/>        </th></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyCenter" colspan="2"><b>Left: Ideal glass with green transmission tint. Right: Ideal glass with blue reflection and green transmission tint.</b>   </td></tr>
</table>
<h2><a class="anchor" id="abraded_opaque"></a>
Abraded Opaque</h2>
<p>Real-world surfaces are seldom perfectly smooth. Therefore, <a class="elRef" href="../../Engine/html/classph_1_1_abraded_opaque.html">AbradedOpaque</a> offers a wide range of tweakable microsurface parameters for opaque materials. A popular BRDF model that allows this is the Cook-Torrance microfacet BRDF <a class="el" href="citelist.html#CITEREF_Cook:1981:Reflectance">[3]</a>. For the normal distribution function (NDF), we use the <a class="elRef" href="../../Engine/html/classph_1_1_trowbridge_reitz.html">Trowbridge-Reitz model</a> (also known as the GGX model) <a class="el" href="citelist.html#CITEREF_Trowbridge:1975:Average">[18]</a> by default, as it has been shown to match experimental data well. The model can use both <a class="elRef" href="../../Engine/html/classph_1_1_exact_conductor_fresnel.html">exact</a> and <a class="elRef" href="../../Engine/html/classph_1_1_schlick_approx_conductor_fresnel.html">approximated</a> versions of the Fresnel equation <a class="el" href="citelist.html#CITEREF_Greve:2006:Reflections">[5]</a>. In the case of the exact Fresnel equation, measured spectral index of refraction (IoR) can be used (complex IoR is also supported). <a href="https://refractiveindex.info/">This site</a> has a good collection of measured IoR data.</p>
<table class="markdownTable">
<tr class="markdownTableHead">
<th class="markdownTableHeadCenter"><img src="microfacet_gold_0p1.jpg" alt="" class="inline" title="Microfacet Gold"/>       </th><th class="markdownTableHeadCenter"><img src="microfacet_gold_0p5.jpg" alt="" class="inline" title="Microfacet Gold Rougher"/>        </th></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyCenter" colspan="2"><b>Left: Gold with roughness = 0.1. Right: Gold with roughness = 0.5.</b>   </td></tr>
</table>
<div class="image">
<img src="microfacet_gold_1p0.jpg" alt="" width="50%"/>
<div class="caption">
Gold with roughness = 1.0. Note the retroreflective effect at grazing angles due to the high roughness value.</div></div>
    <p>NDFs other than Trowbridge-Reitz are also supported, see <a class="elRef" href="../../Engine/html/namespaceph.html#adc27a9e67654dae7969a693c5a098a3a">here</a>. Roughness values are often remapped to the \( \alpha \) parameter in the NDF, we have implementations for some <a class="elRef" href="../../Engine/html/namespaceph.html#a0cdbba0cf5ad47eb99146ac3b2d912e7">common mappings</a>. The masking and shadowing terms in a microfacet BRDF often have some kind of correlation <a class="el" href="citelist.html#CITEREF_Heitz:2014:Microfacet">[8]</a>, and we provide some <a class="elRef" href="../../Engine/html/namespaceph.html#a0c0fa392ffb753722f245acbe724095f">common forms</a> to choose from.</p>
<p>The generalized form of Trowbridge-Reitz is anisotropic, which is described thoroughly in Disney's course note at SIGGRAPH 2012 <a class="el" href="citelist.html#CITEREF_Burley:2012:Physicallybased">[2]</a>. For anisotropic NDFs, we have an implementation of the \( D_{GTR} \) variant with \( \gamma = 2 \). A sharp edge of anisotropic materials is that they require properly mapped UV coordinates across the mesh in order to form a local basis around the shading point, which is necessary for determining the value of the NDF. Below is some renderings of anisotropic microfacet materials:</p>
<table class="markdownTable">
<tr class="markdownTableHead">
<th class="markdownTableHeadCenter"><img src="microfacet_titanium_0p05_0p25.jpg" alt="" class="inline" title="Microfacet Titanium"/>       </th><th class="markdownTableHeadCenter"><img src="microfacet_titanium_0p25_0p05.jpg" alt="" class="inline" title="Microfacet Titanium"/>        </th></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyCenter" colspan="2"><b>Brushed titanium. Left: Roughness = (0.05, 0.25). Right: Roughness = (0.25, 0.05).</b>   </td></tr>
</table>
<div class="image">
<img src="anisotropic_0_0p2.jpg" alt=""/>
<div class="caption">
A comparison between anisotropic microfacet BRDFs. Left: Roughness = (0.2, 0). Middle: Roughness = (0.2, 0.2). Right: Roughness = (0, 0.2).</div></div>
    <h2><a class="anchor" id="abraded_translucent"></a>
Abraded Translucent</h2>
<p>Since the aforementioned microfacet theory is just a description of microgeometry, it can be applied to the case of light transmission with some modifications <a class="el" href="citelist.html#CITEREF_Walter:2007:Microfacet">[20]</a>. The most prominent change is that for each shading point, we now need to trace two paths instead of one, for reflection and transmission, respectively. Much of the code for handling reflection can be reused, but not for transmission (it requires an additional BTDF). This is one of the reasons why the abraded <a class="elRef" href="../../Engine/html/classph_1_1_abraded_opaque.html">opaque</a> and <a class="elRef" href="../../Engine/html/classph_1_1_abraded_translucent.html">translucent</a> materials do not share a common base class. As with its <a class="el" href="#abraded_opaque">opaque variant</a>, this material also supports VNDF-based sampling <a class="el" href="citelist.html#CITEREF_Dupuy:2023:Sampling">[4]</a> in addition to ordinary NDF-based sampling. This microfacet-based translucent material is effective for modeling frosted glass. Currently, only single-bounce lighting inside the microgeometry is simulated. Multi-bounce lighting within the microgeometry will be incorporated in the near future.</p>
<table class="markdownTable">
<tr class="markdownTableHead">
<th class="markdownTableHeadCenter"><img src="microfacet_glass_0p2.jpg" alt="" class="inline" title="Microfacet Glass"/>       </th><th class="markdownTableHeadCenter"><img src="microfacet_glass_0p6.jpg" alt="" class="inline" title="Microfacet Glass Rougher"/>        </th></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyCenter" colspan="2"><b>Frosted glass. Left: Roughness = 0.2. Right: Roughness = 0.6.</b>   </td></tr>
</table>
<h2><a class="anchor" id="autotoc_md33"></a>
Binary Mixed Surface</h2>
<p>Being able to <a class="elRef" href="../../Engine/html/classph_1_1_binary_mixed_surface_material.html">mix two different materials</a> greatly increases the diversity of possible materials. The mixing process can be nested, e.g., a mixed material can participate in another mixing process just like an ordinary, single material. Some material appearances, such as plastic and ceramic, can be achieved easily by this model (though it may not be accurate). As many other material attributes in Photon, the weighting factor for the mix can be mapped by an image.</p>
<table class="markdownTable">
<tr class="markdownTableHead">
<th class="markdownTableHeadCenter"><img src="ceramic_0p8_0p2_float.jpg" alt="" class="inline" title="Ceramic Float Mix"/>       </th><th class="markdownTableHeadCenter"><img src="ceramic_0p8_0p2_textured.jpg" alt="" class="inline" title="Ceramic Texture Mix"/>        </th></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyCenter" colspan="2"><b>Left: Mixing matte opaque with abraded opaque materials with a (0.8, 0.2) weighting, producing a ceramic-like appearance. Right: Using an image to weight the same two materials.</b>   </td></tr>
</table>
<h2><a class="anchor" id="autotoc_md34"></a>
Layered Material</h2>
<p>Layered material models have become prevalent in the graphics community for quite some time. The solution by Jakob et al. <a class="el" href="citelist.html#CITEREF_Jakob:2014:Comprehensive">[9]</a> requires larger memory space to store some BSDFs, while the model developed by Weidlich et al. <a class="el" href="citelist.html#CITEREF_Weidlich:2007:Arbitrarily">[21]</a> requires little space but does not obey the law of energy conservation. Laurent Belcour developed a series of atomic statistical operators for simulating light transport within layers <a class="el" href="citelist.html#CITEREF_Belcour:2018:Efficient">[1]</a>. His solution is still not exact but is energy conserving, space efficient, and plays well with texture mapping. Photon has an implementation of Belcour's layered material model, which follows his original implementation closely, with some refactoring and improvements on numerical robustness.</p>
<table class="markdownTable">
<tr class="markdownTableHead">
<th class="markdownTableHeadCenter"><img src="belcour_teaser.jpg" alt="" class="inline" title="Belcour Mug"/>       </th><th class="markdownTableHeadCenter"><img src="belcour_grease_iron.jpg" alt="" class="inline" title="Belcour Grease Iron"/>        </th></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyCenter" colspan="2"><b>Left: Simulating a mug material. Right: Polished iron with 2 layers of coating. One of the coating layer simulates grease with volumetric light transport.</b>   </td></tr>
</table>
<div class="image">
<img src="beclour_mug_param.jpg" alt="" width="30%"/>
<div class="caption">
Note that the mug material (the left image above) uses the same parameters taken from the teaser image of Belcour's paper.</div></div>
    <h1><a class="anchor" id="geometry"></a>
Geometry</h1>
<p>To represent a ball in the scene, you can use a <a class="elRef" href="../../Engine/html/classph_1_1_g_sphere.html">sphere geometry</a>; for a piece of paper on a table, a <a class="elRef" href="../../Engine/html/classph_1_1_g_rectangle.html">rectangle geometry</a> can model it pretty well. Geometries are the core components of a scene, providing the structural framework for all other elements like materials, lights and physical motions. In the following sections, we will explore some common types of geometries available in Photon, giving you a rough idea of how these foundational elements are constructed and used in rendering.</p>
<h2><a class="anchor" id="autotoc_md35"></a>
Intersectable and Primitive</h2>
<p>When a <a class="elRef" href="../../Engine/html/classph_1_1_geometry.html">Geometry</a> is defined in the scene, it will be converted to an <a class="elRef" href="../../Engine/html/classph_1_1_intersectable.html">Intersectable</a> before the actual rendering process begins. The renderer further treat some intersectable types as <a class="elRef" href="../../Engine/html/classph_1_1_primitive.html">Primitive</a> from which a richer set of functionalities are expected. In a similar way, some intersectable types are being classified as <a class="elRef" href="../../Engine/html/classph_1_1_intersector.html">Intersector</a> and they typically act as an aggregate of intersectables.</p>
<dl class="section note"><dt>Note</dt><dd>While these distinctions are not crucial from a user's perspective, they are important for developers or researchers who wish to delve deeper into the source code. Understanding these classifications also makes engine modifications easier.</dd></dl>
<h2><a class="anchor" id="autotoc_md36"></a>
Basic Shapes</h2>
<p>These are common shapes to have in a renderer and are frequently found in other render engines as well. They are useful not only for modeling objects but also for defining the shape of light sources (or anything that requires a shape). Tessellation is also supported on some types of geometries. We start by introducing the rectangular shape, which can be created by <a class="elRef" href="../../Engine/html/classph_1_1_g_rectangle.html">rectangle geometry</a>.</p>
<div class="image">
<img src="rectangle.jpg" alt=""/>
<div class="caption">
Rectangle lies on the XY-plane by default. With UV in [0, 1] across the whole surface.</div></div>
    <p>We also have freestanding triangular shape which can be created by <a class="elRef" href="../../Engine/html/classph_1_1_g_triangle.html">triangle geometry</a>.</p>
<div class="image">
<img src="triangle.jpg" alt=""/>
<div class="caption">
Triangle has customizable UVs and normals.</div></div>
    <dl class="section note"><dt>Note</dt><dd>It is not recommended to build complex shapes out of multiple freestanding triangle geometries unless you have good reasons to do so. It is highly memory inefficient.</dd></dl>
<p>There is also spherical shape created by <a class="elRef" href="../../Engine/html/classph_1_1_g_sphere.html">sphere geometry</a>. The sphere generated is a true sphere by default (not a triangle mesh counterpart).</p>
<div class="image">
<img src="sphere.jpg" alt=""/>
<div class="caption">
An example sphere shape.</div></div>
    <p>Using <a class="elRef" href="../../Engine/html/classph_1_1_g_cuboid.html">cuboid geometry</a>, you can build a cube or box shape. Cuboids are axis-aligned bounding boxes (AABB) in their local space and allow for variable extents, making them more flexible than standard cubes.</p>
<div class="image">
<img src="cuboid.jpg" alt=""/>
<div class="caption">
A Cuboid (in fact a cube in this image).</div></div>
    <p>A bonus to have cuboids is that voxel games like <a href="https://en.wikipedia.org/wiki/Minecraft">Minecraft</a> can be rendered easily. Below is a work-in-progress render of a Minecraft level parser that tries to translate in-game data into PSDL:</p>
<div class="image">
<img src="textured_mc.jpg" alt=""/>
<div class="caption">
Test render of a Minecraft level parser.</div></div>
    <p>Almost all shapes are built from triangle meshes for a typical scene. Games, modeling programs and other applications typically use triangle mesh to represent arbitrary 3-D shapes. It is basically a collection of triangles grouped in a way that approximates a target shape. It can be created by <a class="elRef" href="../../Engine/html/classph_1_1_g_triangle_mesh.html">triangle mesh geometry</a> or <a class="elRef" href="../../Engine/html/classph_1_1_g_ply_polygon_mesh.html">polygon mesh</a>. The polygon mesh variant is a more memory efficient representation of a triangle mesh and supports customizable vertex layout and arbitrary index bit precision. Binary storage format is also supported (<a href="https://en.wikipedia.org/wiki/PLY_(file_format)">PLY format</a>).</p>
<div class="image">
<img src="triangle_mesh.jpg" alt=""/>
<div class="caption">
Stanford bunny built from a triangle mesh.</div></div>
    <h2><a class="anchor" id="autotoc_md37"></a>
Masking</h2>
<h2><a class="anchor" id="autotoc_md38"></a>
Advanced Shapes</h2>
<p>We also support some interesting shapes such as wave and fractals. These special geometries are sometimes useful for modeling a scene. One of them is the <a href="https://en.wikipedia.org/wiki/Menger_sponge">Menger sponge</a>, a famous fractal shape. It can be created by <a class="elRef" href="../../Engine/html/classph_1_1_g_menger_sponge.html">Menger sponge geometry</a>.</p>
<div class="image">
<img src="menger_sponge.jpg" alt=""/>
<div class="caption">
A fractal geometry (Menger sponge).</div></div>
    <p>The wave shape is basically a cuboid with its top surface tessellated according to a superposition of 2-D sine and cosine functions. It can be created by <a class="elRef" href="../../Engine/html/classph_1_1_g_wave.html">wave geometry</a>.</p>
<div class="image">
<img src="045_water_dragon_color_light.jpg" alt="" width="80%"/>
<div class="caption">
A triangle mesh submerged inside a wave geometry.</div></div>
    <p>These advanced shapes add versatility to scene modeling in Photon.</p>
<h2><a class="anchor" id="autotoc_md39"></a>
Acceleration Structure</h2>
<h1><a class="anchor" id="autotoc_md40"></a>
Light</h1>
<p>Lighting is a crucial component in rendering a virtual world. Photon provides various types of light sources and emphasizes the use of physically based units for input parameters. This approach is supported by three primary reasons:</p>
<ul>
<li>Photon is designed as a physically based renderer, using physically based inputs ensures a natural and accurate conveyance of intent.</li>
<li>Users can expect consistent renderings between different versions of the software with identical inputs.</li>
<li>It is easier to validate the results against real world measurements.</li>
</ul>
<p>In Photon, lights are represented by a base actor type <a class="elRef" href="../../Engine/html/classph_1_1_a_light.html">ALight</a>. These light actors will be converted to a series of cooked data, including <a class="elRef" href="../../Engine/html/classph_1_1_primitive.html">Primitive</a> (the geometric shape of the light), <a class="elRef" href="../../Engine/html/classph_1_1_surface_optics.html">SurfaceOptics</a> (the surface material of the light), <a class="elRef" href="../../Engine/html/classph_1_1_emitter.html">Emitter</a> (the energy source of the light), etc. The following sections give a brief overview on the types of light sources implemented in Photon.</p>
<h2><a class="anchor" id="autotoc_md41"></a>
Area Light</h2>
<p>Photon currently supports <a class="elRef" href="../../Engine/html/classph_1_1_a_rectangle_light.html">rectangular</a> and <a class="elRef" href="../../Engine/html/classph_1_1_a_sphere_light.html">spherical</a> area lights. An <a class="elRef" href="../../Engine/html/classph_1_1_a_area_light.html">area light</a> emits energy uniformly across its surface, producing a visually pleasing transition between umbra and penumbra. The amount of energy emitted is specified in <a href="https://en.wikipedia.org/wiki/Watt">Watt</a>.</p>
<div class="image">
<img src="rectangle_light_175W.jpg" alt=""/>
<div class="caption">
A 175 W rectangle light.</div></div>
    <p>A spherical area light illuminating the same teapots:</p>
<div class="image">
<img src="sphere_light_300W.jpg" alt=""/>
<div class="caption">
A 300 W sphere light (1 m radius).</div></div>
    <h2><a class="anchor" id="autotoc_md42"></a>
Point Light</h2>
<p>In Photon, <a class="elRef" href="../../Engine/html/classph_1_1_a_point_light.html">point light</a> is implemented as a special spherical area light. This may suprise some, as we treat point lights as a subtype of area lights. The rationale is that traditional point light introduces an additional singularity in the rendering equation, and we already have quite a lot of singularities in the system, e.g., pinhole camera and mirror reflection/transmission. Moreover, traditional point lights are not physically possible anyway. Each singularity often requires special routines or conditions to handle properly, and managing them all as the renderer's complexity grows can be cumbersome. The idea is to give point lights a finite surface area, typically a sphere with a 1- to 3-cm diameter resembling the size of common light bulbs. <a href="http://www.nextlimit.com/maxwell/">Maxwell Renderer</a> has adopted a similar design decision, although the reason behind it may differ. Below is a 300 W point light, emitting roughly the same brightness as the 300 W sphere light shown earlier:</p>
<div class="image">
<img src="point_light_300W.jpg" alt=""/>
<div class="caption">
A 300 W point light (0.5 cm radius).</div></div>
    <h2><a class="anchor" id="autotoc_md43"></a>
Model Light</h2>
<p><a class="elRef" href="../../Engine/html/classph_1_1_a_model_light.html">Model light</a> can be considered a superset of <a class="elRef" href="../../Engine/html/classph_1_1_a_area_light.html">area light</a>. A key difference between them is that while area light need to have a constant emission profile, model light lifted this limitation and allow using variable emission profiles (such as <a class="elRef" href="../../Engine/html/classph_1_1_image.html">images</a>) on arbitrary <a class="elRef" href="../../Engine/html/classph_1_1_geometry.html">geometry</a>.</p>
<div class="image">
<img src="model_light.jpg" alt=""/>
<div class="caption">
A model light in action: the textured teapot is emitting energy from its surface.</div></div>
    <h2><a class="anchor" id="autotoc_md44"></a>
IES Light Profiles</h2>
<p>An IES light profile stores the distribution of emitted energy of a light fixture. The majority of the datasets are provided by lighting manufacturers and are particularly useful for interior design and architecture visualization. Most commercial renderers can parse and render IES-based lights, and even some game engines support them. Photon does not treat IES light profiles as energy distribution functions, rather, they are interpreted as energy attenuating filters (effectively normalizing energy values to [0, 1], see <a class="elRef" href="../../Engine/html/classph_1_1_a_ies_attenuated_light.html">IES attenuated light</a>). This approach allows users to adjust total energy emitted by a light source freely, without being constrained by the absolute energy values stroed in the IES light profile. Still, it is always possible to extract the maximum energy density from an IES data file then applying the attenuation to faithfully reproduce the light fixture in the renderer.</p>
<div class="image">
<img src="ies_point_light_300W.jpg" alt=""/>
<div class="caption">
An interesting IES light profile applied on a 300 W point light.</div></div>
    <h2><a class="anchor" id="autotoc_md45"></a>
Sky Dome</h2>
<p>A powerful method of lighting is image based techniques, also known as environment map or HDRI lighting. The main idea is to assume energy from far regions can be tabulated with direction vectors as entries. Lighting up a scene with HDRIs is usually the fastest and the most effective way to achieve natural-looking results. In photon, we call this type of light source a <a class="elRef" href="../../Engine/html/classph_1_1_a_dome.html">dome light</a>. For a nice collection of HDR environment maps, visit <a href="https://hdrihaven.com/">https://hdrihaven.com/</a>. Shown below is the same demo scene lit with an <a class="elRef" href="../../Engine/html/classph_1_1_a_image_dome.html">image dome</a> source:</p>
<div class="image">
<img src="env_light.jpg" alt=""/>
</div>
    <p>Another approach to lighting a scene is to physically model the energy coming from the sky. When it comes to sun-sky models, a classic example is the model by Preetham et al. <a class="el" href="citelist.html#CITEREF_Preetham:1999:Practical">[16]</a> This model approximates the absolute value of radiant energy coming from the sky using a set of formulae. Given the latitude, longitude, date and time of the target location, we can analytically generate the sun and sky using <a class="elRef" href="../../Engine/html/classph_1_1_a_preetham_dome.html">Preetham dome</a>.</p>
<div class="image">
<img src="preetham_light.jpg" alt=""/>
<div class="caption">
New York City illuminated by the morning sunlight at Helsinki (60.2° N, 24.9° E) on Dec. 20.</div></div>
    <h1><a class="anchor" id="autotoc_md46"></a>
Sample Source</h1>
<h2><a class="anchor" id="autotoc_md47"></a>
Sample Generator</h2>
<h1><a class="anchor" id="autotoc_md48"></a>
Toolset</h1>
<p>Photon comes with a series of tools to facilitate scene rendering and management.</p>
<h2><a class="anchor" id="autotoc_md49"></a>
Film Merger</h2>
<p>This is a tool for combining render output from multiple render sessions. A typical use case is the same scene being rendered by multiple machines, each producing a different render output. We call these render outputs "films". Multiple films can be merged into a single film in various ways. For example, statistically independent films can be merged into a more converged film with reduced noise and bias. Visit the <a class="elRef" href="../../FilmMerger/html/index.html#photon_film_merger_readme">tool's documentation page</a> for more information. </p>
</div></div><!-- contents -->
</div><!-- PageDoc -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="footer">Generated by <a href="https://www.doxygen.org/index.html"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.11.0 </li>
  </ul>
</div>
</body>
</html>
